"""
Provide helper classes/functions to execute ELMo.
"""
# pylint: disable=no-self-use
# pylint: disable=arguments-differ

from typing import List, Tuple, Optional, Dict, Union, Any

import torch
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence

from pytorch_fast_elmo import (  # type: ignore
        ElmoCharacterEncoderRestorer, ElmoWordEmbeddingRestorer, ElmoLstmRestorer, ScalarMix,
)


def _make_bos_eos(
        character: int,
        padding_character: int,
        beginning_of_word_character: int,
        end_of_word_character: int,
        max_word_length: int,
) -> List[int]:
    char_ids = [padding_character] * max_word_length
    char_ids[0] = beginning_of_word_character
    char_ids[1] = character
    char_ids[2] = end_of_word_character
    return [c + 1 for c in char_ids]


def _bind_cpp_extension_parameters(
        py_module: torch.nn.Module,
        cpp_module: Any,
        param_prefix: str = '',
        override: bool = False,
        only_trainable: bool = False,
) -> None:
    if isinstance(cpp_module, torch.nn.Module):
        raise TypeError('cpp_module should not be torch.nn.Module.')

    prefix = 'cpp_ext_' + param_prefix
    for name, tensor in cpp_module.named_parameters().items():
        if only_trainable and not tensor.requires_grad:
            continue

        param_name = (prefix + name).replace('.', '_')
        if override and hasattr(py_module, param_name):
            delattr(py_module, param_name)

        py_module.register_parameter(
                param_name,
                torch.nn.Parameter(tensor, requires_grad=tensor.requires_grad),
        )


class FastElmoBase(torch.nn.Module):  # type: ignore

    def __init__(
            self,

            # Generated by bilm-tf.
            options_file: str,
            weight_file: str,

            # Controls the behavior of restorer.
            # Note: following options should match the `options_file`.
            # Char CNN.
            disable_char_cnn: bool = False,
            char_cnn_requires_grad: bool = False,

            # Word Embedding.
            disable_word_embedding: bool = True,
            word_embedding_requires_grad: bool = False,

            # The forward LSTM.
            disable_forward_lstm: bool = False,
            forward_lstm_requires_grad: bool = False,

            # The backward LSTM.
            disable_backward_lstm: bool = False,
            backward_lstm_requires_grad: bool = False,

            # Provide the BOS/EOS representations of shape `(projection_dim,)`
            # if char CNN is disabled.
            lstm_bos_repr: Optional[torch.Tensor] = None,
            lstm_eos_repr: Optional[torch.Tensor] = None,

            # Controls the behavior of `ScalarMix`.
            disable_scalar_mix: bool = False,
            num_output_representations: int = 1,
            output_representation_dropout: float = 0.0,
            scalar_mix_parameters: Optional[List[float]] = None,
            do_layer_norm: bool = False,
    ) -> None:
        super().__init__()

        self.disable_char_cnn = disable_char_cnn
        self.disable_word_embedding = disable_word_embedding
        self.disable_forward_lstm = disable_forward_lstm
        self.disable_backward_lstm = disable_backward_lstm
        self.disable_scalar_mix = disable_scalar_mix

        # Char CNN.
        self.char_cnn_restorer = ElmoCharacterEncoderRestorer(
                options_file,
                weight_file,
        )
        if not disable_char_cnn:
            self.char_cnn = self.char_cnn_restorer.restore(requires_grad=char_cnn_requires_grad)

        # Word Embedding.
        self.word_embedding_restorer = ElmoWordEmbeddingRestorer(
                options_file,
                weight_file,
        )
        if not disable_word_embedding:
            # Not a cpp extension.
            self.word_embedding = self.word_embedding_restorer.restore(
                    requires_grad=word_embedding_requires_grad,)

        # LSTM.
        self.lstm_restorer = ElmoLstmRestorer(
                options_file,
                weight_file,
        )
        self.forward_lstm, self.backward_lstm = self.lstm_restorer.restore(
                enable_forward=not disable_forward_lstm,
                forward_requires_grad=forward_lstm_requires_grad,
                enable_backward=not disable_backward_lstm,
                backward_requires_grad=backward_lstm_requires_grad,
        )
        # Cache BOS/EOS reprs.
        if not (disable_forward_lstm and disable_backward_lstm):
            if disable_char_cnn:
                if lstm_bos_repr is None or lstm_eos_repr is None:
                    raise ValueError('BOS/EOS not provided.')
                self.lstm_bos_repr = lstm_bos_repr
                self.lstm_eos_repr = lstm_eos_repr

            else:
                # [<bow>, <bos/eos>, <eow>, max(kernal)...]
                max_characters_per_token = max(
                        kernal_size for kernal_size, _ in self.char_cnn_restorer.filters)
                max_characters_per_token += 3

                bos_ids = _make_bos_eos(
                        256,  # bos
                        260,  # pad
                        258,  # bow
                        259,  # eow
                        max_characters_per_token,
                )
                eos_ids = _make_bos_eos(
                        257,  # eos
                        260,  # pad
                        258,  # bow
                        259,  # eow
                        max_characters_per_token,
                )

                # On CPU.
                bos_eos = torch.LongTensor([bos_ids, eos_ids])
                with torch.no_grad():
                    bos_eos_reprs = self.char_cnn(bos_eos)

                self.lstm_bos_repr = bos_eos_reprs[0]
                self.lstm_eos_repr = bos_eos_reprs[1]

        # ScalarMix
        if not disable_scalar_mix:
            self.scalar_mixes: List[ScalarMix] = []

            if scalar_mix_parameters is None:
                scalar_mix_parameters = []

            for _ in range(num_output_representations):
                scalar_mix = ScalarMix(
                        # char cnn + lstm.
                        self.lstm_restorer.num_layers + 1,
                        do_layer_norm=do_layer_norm,
                        initial_scalar_parameters=scalar_mix_parameters,
                        trainable=not scalar_mix_parameters,
                )
                self.scalar_mixes.append(scalar_mix)

            self.repr_dropout = None
            if output_representation_dropout > 0.0:
                self.repr_dropout = torch.nn.Dropout(p=output_representation_dropout)

        # Bind CPU parameters.
        self._bind_parameters()

    def _bind_parameters(self, override: bool = False) -> None:
        # Since `ElmoCharacterEncoder`, `StatefulUnidirectionalLstm`, `ScalarMix` are not
        # instances of `torch.nn.Module`, we need to bind the parameters manually.
        if not self.disable_char_cnn:
            _bind_cpp_extension_parameters(
                    self,
                    self.char_cnn,
                    'char_cnn_',
                    override=override,
                    only_trainable=True,
            )

        if not self.disable_forward_lstm:
            _bind_cpp_extension_parameters(
                    self,
                    self.forward_lstm,
                    'forward_lstm_',
                    override=override,
                    only_trainable=True,
            )
        if not self.disable_backward_lstm:
            _bind_cpp_extension_parameters(
                    self,
                    self.backward_lstm,
                    'backward_lstm_',
                    override=override,
                    only_trainable=True,
            )

        if not self.disable_scalar_mix:
            for idx, scalar_mix in enumerate(self.scalar_mixes):
                _bind_cpp_extension_parameters(
                        self,
                        scalar_mix,
                        f'scalar_mix_{idx}_',
                        override=override,
                        only_trainable=True,
                )

    def cuda(self, device=None):  # type: ignore
        # Move all cpp exntensions to GPU.
        if not self.disable_char_cnn:
            self.char_cnn.cuda()

        if not self.disable_forward_lstm:
            self.forward_lstm.cuda()
        if not self.disable_backward_lstm:
            self.backward_lstm.cuda()

        if not self.disable_scalar_mix:
            for scalar_mix in self.scalar_mixes:
                scalar_mix.cuda()

        # Override parameter bindings.
        self._bind_parameters(override=True)

        # TODO: find a way to pass `device` to cpp extension.
        return super().cuda()

    def cpu(self):  # type: ignore
        if self._get_lstm_device() >= 0:
            raise RuntimeError('Going to GPU is a one-way ticket for now.')
        else:
            return self

        # TODO: Investigate.
        #
        # # Move all cpp exntensions to CPU.
        # if not self.disable_char_cnn:
        #     self.char_cnn.cpu()
        #
        # if not self.disable_forward_lstm:
        #     self.forward_lstm.cpu()
        # if not self.disable_backward_lstm:
        #     self.backward_lstm.cpu()
        #
        # if not self.disable_scalar_mix:
        #     for scalar_mix in self.scalar_mixes:
        #         scalar_mix.cpu()
        #
        # # Also, move BOS/EOS back to CPU.
        # if not (self.disable_forward_lstm and self.disable_backward_lstm):
        #     self.lstm_bos_repr = self.lstm_bos_repr.cpu()
        #     self.lstm_eos_repr = self.lstm_eos_repr.cpu()
        #
        # # Override parameter bindings.
        # self._bind_parameters(override=True)
        #
        # return super().cpu()

    def _get_lstm_device(self) -> int:
        cpp_ext = None
        if not self.disable_forward_lstm:
            cpp_ext = self.forward_lstm
        elif not self.disable_backward_lstm:
            cpp_ext = self.backward_lstm

        # Assume `cpp_ext` is not None.
        assert cpp_ext is not None
        tensor = cpp_ext.parameters()[0]
        return -1 if not tensor.is_cuda else tensor.get_device()  # type: ignore

    def get_batched_lstm_bos_eos_repr(self, attr_name: str, batch_size: int) -> PackedSequence:
        tensor = getattr(self, attr_name)

        if not tensor.is_cuda:
            # Move to GPU permanently.
            device = self._get_lstm_device()
            if device >= 0:
                tensor = tensor.cuda(device)
                setattr(self, attr_name, tensor)

        batched = tensor.unsqueeze(0).expand(batch_size, -1)
        return PackedSequence(batched, torch.LongTensor([batch_size]))

    def get_batched_lstm_bos_repr(self, batch_size: int) -> PackedSequence:
        return self.get_batched_lstm_bos_eos_repr('lstm_bos_repr', batch_size)

    def get_batched_lstm_eos_repr(self, batch_size: int) -> PackedSequence:
        return self.get_batched_lstm_bos_eos_repr('lstm_eos_repr', batch_size)

    def call_forward_lstm_bos(self, batch_size: int) -> None:
        batched = self.get_batched_lstm_bos_repr(batch_size)
        with torch.no_grad():
            self.forward_lstm(batched.data, batched.batch_sizes)

    def call_forward_lstm_eos(self, batch_size: int) -> None:
        batched = self.get_batched_lstm_eos_repr(batch_size)
        with torch.no_grad():
            self.forward_lstm(batched.data, batched.batch_sizes)

    def call_backward_lstm_bos(self, batch_size: int) -> None:
        batched = self.get_batched_lstm_bos_repr(batch_size)
        with torch.no_grad():
            self.backward_lstm(batched.data, batched.batch_sizes)

    def call_backward_lstm_eos(self, batch_size: int) -> None:
        batched = self.get_batched_lstm_eos_repr(batch_size)
        with torch.no_grad():
            self.backward_lstm(batched.data, batched.batch_sizes)

    def call_char_cnn(self, inputs: PackedSequence) -> PackedSequence:
        """
        Char CNN.
        """
        output_data = self.char_cnn(inputs.data)
        return PackedSequence(output_data, inputs.batch_sizes)

    def call_word_embedding(self, inputs: PackedSequence) -> PackedSequence:
        """
        Word embedding.
        """
        output_data = self.word_embedding(inputs.data)
        return PackedSequence(output_data, inputs.batch_sizes)

    def call_forward_lstm(
            self,
            inputs: PackedSequence,
            bos_eos: bool = True,
    ) -> List[PackedSequence]:
        """
        Forward LSTM.
        """
        if bos_eos:
            max_batch_size = int(inputs.batch_sizes.data[0])
            # BOS.
            self.call_forward_lstm_bos(max_batch_size)

        # Feed inputs.
        outputs, _ = self.forward_lstm(inputs.data, inputs.batch_sizes)

        if bos_eos:
            # EOS.
            self.call_forward_lstm_eos(max_batch_size)

        # To list of `PackedSequence`.
        return [PackedSequence(output, inputs.batch_sizes) for output in outputs]

    def call_backward_lstm(
            self,
            inputs: PackedSequence,
            bos_eos: bool = True,
    ) -> List[PackedSequence]:
        """
        Backward LSTM.
        """
        if bos_eos:
            max_batch_size = int(inputs.batch_sizes.data[0])
            # EOS.
            self.call_backward_lstm_eos(max_batch_size)

        # Feed inputs.
        outputs, _ = self.backward_lstm(inputs.data, inputs.batch_sizes)

        if bos_eos:
            # BOS.
            self.call_backward_lstm_bos(max_batch_size)

        # To list of `PackedSequence`.
        return [PackedSequence(output, inputs.batch_sizes) for output in outputs]

    def call_bilstm(
            self,
            inputs: PackedSequence,
            bos_eos: bool = True,
    ) -> List[Tuple[PackedSequence, PackedSequence]]:
        """
        BiLSTM.
        """
        forward_seqs = self.call_forward_lstm(inputs, bos_eos)
        backward_seqs = self.call_backward_lstm(inputs, bos_eos)

        return list(zip(forward_seqs, backward_seqs))

    def concat_packed_sequences(
            self,
            packed_sequences: List[Tuple[PackedSequence, PackedSequence]],
    ) -> List[PackedSequence]:
        """
        Concatenate the outputs of fwd/bwd lstms.
        """
        return [
                PackedSequence(
                        torch.cat([fwd.data, bwd.data], dim=-1),
                        fwd.batch_sizes,
                ) for fwd, bwd in packed_sequences
        ]

    def combine_char_cnn_and_bilstm_outputs(
            self,
            char_cnn_packed: PackedSequence,
            bilstm_packed: List[PackedSequence],
    ) -> List[PackedSequence]:
        """
        Combine the outputs of Char CNN & BiLSTM for scalar mix.
        """
        # Simply duplicate the output of char cnn.
        duplicated_char_cnn_packed = PackedSequence(
                torch.cat([char_cnn_packed.data, char_cnn_packed.data], dim=-1),
                char_cnn_packed.batch_sizes,
        )

        combined = [duplicated_char_cnn_packed]
        combined.extend(bilstm_packed)
        return combined

    def call_scalar_mix(self, packed_sequences: List[PackedSequence]) -> List[PackedSequence]:
        """
        Scalar Mix.
        """
        reprs = []
        for scalar_mix in self.scalar_mixes:
            mixed = scalar_mix([inputs.data for inputs in packed_sequences])
            if self.repr_dropout is not None:
                mixed = self.repr_dropout(mixed)
            reprs.append(PackedSequence(mixed, packed_sequences[0].batch_sizes))
        return reprs

    def pack_inputs(self, inputs: torch.Tensor) -> PackedSequence:
        """
        Pack inputs of shape `(batch_size, timesteps, x)` or `(batch_size, timesteps)`.
        Padding value should be 0.
        """
        if inputs.dim() == 2:
            lengths = (inputs > 0).long().sum(dim=-1)
        elif inputs.dim() == 3:
            lengths = ((inputs > 0).long().sum(dim=-1) > 0).long().sum(dim=-1)

        return pack_padded_sequence(inputs, lengths, batch_first=True)

    def unpack_outputs(
            self,
            inputs: PackedSequence,
            skip_mask: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Unpack the final result and return `(tensor, mask)`.
        """
        tensor, lengths = pad_packed_sequence(inputs, batch_first=True)
        if skip_mask:
            return tensor, None
        if tensor.is_cuda:
            lengths = lengths.cuda()

        ones = lengths.new_ones(tensor.shape[0], tensor.shape[1], dtype=torch.long)
        range_tensor = ones.cumsum(dim=-1)
        mask = (lengths.unsqueeze(1) >= range_tensor).long()
        return tensor, mask

    def forward(self):  # type: ignore
        raise NotImplementedError()


class FastElmo(FastElmoBase):  # type: ignore

    def forward(
            self,
            inputs: torch.Tensor,
            bos_eos: bool = True,
    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """
        The full workflow (same as AllenNLP).
        """
        # Convert to `PackedSequence`
        packed_inputs = self.pack_inputs(inputs)

        # Char CNN.
        char_repr = self.call_char_cnn(packed_inputs)
        # BiLSTM.
        bilstm_repr = self.call_bilstm(char_repr, bos_eos)
        # Scalar Mix.
        conbimed_repr = self.combine_char_cnn_and_bilstm_outputs(
                char_repr,
                self.concat_packed_sequences(bilstm_repr),
        )
        mixed_reprs = self.call_scalar_mix(conbimed_repr)

        # Unpack.
        first_mixed_repr_unpacked, mask = self.unpack_outputs(mixed_reprs[0])
        elmo_representations = [first_mixed_repr_unpacked]
        for mixed_repr in mixed_reprs[1:]:
            mixed_repr_unpacked, _ = self.unpack_outputs(mixed_repr, skip_mask=True)
            elmo_representations.append(mixed_repr_unpacked)

        return {'elmo_representations': elmo_representations, 'mask': mask}
